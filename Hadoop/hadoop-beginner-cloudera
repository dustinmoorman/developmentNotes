Overview
    - characteristics of large-scale computation
    - pitfalls of distributed systems
    - designing for scalability

By the numbers...
    - Max data in memory: 32GB
    - Max data per comptuer: 12TB
    - Data processed by Google every month: 400PB ... in 2007
    - Average Job size: 180GB

    - time that would take to read sequentially from a disk: 45m

What does it mean?
    - we can process data very quickly but we can read/write it very slowly
    - solution: parallel reads.

 1 HDD = 75 Mb/sec
 1000 HDDs = 75 GB/sec

    - thumbs up

Sharing is slow
    - grid computing, not a new thing.
        - MPI, PVM, Condor ...
    - grid focus: distribute the workload
        - NetApp filer or other SAN drives many compute nodes
    - Modern focus: distribute the _data_
        - Reading 100 GB off a single filer would leave nodes starved - just store data local in lieu of SAN.

Sharing is tricky
    - Exchanging data requires synchronization
        - deadlock becomes a problem
    - Finite bandwidth is available
        - Distributed systems can "drown themselves"
        - Failovers can cause cascading faults
    - Temporal dependencies are complicated
        - Difficult to reason about partial restarts

"Failure is the defining difference between distributed and local programming"

Reliability Demands
    - Support partial failure
        - Total system must support graceful decline in application performance rather than a full halt
    - Data Recoverability
        - If components fail, their workload must be picked up by still-functioning units
    - Individual Recoverability
        - Nodes that fail and restart must be able to rejoin the group activity without a full group restart
    - Consistency
        - Concurrent operations or partial internal failures should not cause externally visible nondeterminism

